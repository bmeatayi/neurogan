"""
Discriminators with multilayer perceptron architecture for conditional GANs

Author: Mohamad Atayi
"""

import torch.nn as nn
import torch
from collections import OrderedDict
import numpy as np


class Discriminator(nn.Module):
    def __init__(self, sp_dim=(1, 1), stim_dim=(1, 1),
                 n_units=[128, 512, 128],
                 mid_act_func=nn.LeakyReLU(0.2, inplace=True),
                 p_drop=None, spectral_norm=False):
        r"""
        Discriminator with multilayer perceptron architecture
        Args:
            sp_dim (tuple): shape of spike data generated by generator
            stim_dim (tuple): shape of stimulation
            n_units (list): list of number of units in each layer
            mid_act_func (torch.nn.functional): activation function in the middle layers
            p_drop (float): probabilty of dropout between 0 and 1 (None for no dropout)
            spectral_norm (bool): Apply spectral normalization on each layer
        """
        super(Discriminator, self).__init__()

        self.nt, self.n_cell = sp_dim
        self.stim_dim = np.prod(stim_dim)
        self.n_units = n_units
        self.p_drop = p_drop
        self.fcin_dim = np.prod(sp_dim)

        dense_layers = OrderedDict()
        if spectral_norm:
            dense_layers['fcin'] = nn.utils.spectral_norm(nn.Linear(self.fcin_dim+self.stim_dim, n_units[0]))
        else:
            dense_layers['fcin'] = nn.Linear(self.fcin_dim + self.stim_dim, n_units[0])

        dense_layers['act0'] = mid_act_func
        for i in range(1, len(n_units)):
            if spectral_norm:
                dense_layers['fc' + str(i)] = nn.utils.spectral_norm(nn.Linear(n_units[i - 1], n_units[i]))
            else:
                dense_layers['fc' + str(i)] = nn.Linear(n_units[i-1], n_units[i])

            if p_drop is not None:
                dense_layers['dropout' + str(i)] = nn.Dropout(p_drop)

            dense_layers['act' + str(i)] = mid_act_func

        if spectral_norm:
            dense_layers['fcout'] = nn.utils.spectral_norm(nn.Linear(n_units[-1], 1))
        else:
            dense_layers['fcout'] = nn.Linear(n_units[-1], 1)

        self.all_layers = nn.Sequential(dense_layers)
        print(self)

    def forward(self, spike, stim):
        x = torch.cat((spike.view(spike.size(0), -1), stim.view(stim.size(0), -1)), -1)
        return self.all_layers(x)
